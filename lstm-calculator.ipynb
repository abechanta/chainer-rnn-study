{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Chain, Variable, optimizers, datasets, iterators, training, serializers, report\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_alphabet = \"0123456789+= \"\n",
    "output_alphabet = \"0123456789 \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(nb_data, k):\n",
    "    a = np.exp(np.random.uniform(np.log(1), np.log(10 ** k), nb_data)).astype(\"i\")\n",
    "    b = np.exp(np.random.uniform(np.log(1), np.log(10 ** k), nb_data)).astype(\"i\")\n",
    "    c = a + b\n",
    "    return a, b, c\n",
    "\n",
    "def encode_in(a, b, k):\n",
    "    alphabet = np.array(list(input_alphabet))\n",
    "    texts = np.array([\"{}+{}=\".format(a_, b_).rjust(k, \" \") for a_, b_ in zip(a, b)])\n",
    "    return np.array([[alphabet == c for c in s] for s in texts]).astype(\"f\")\n",
    "\n",
    "def encode_out(c, k):\n",
    "    texts = np.array([\"{}\".format(c_).ljust(k, \" \") for c_ in c])\n",
    "    return np.array([[output_alphabet.index(c) for c in s] for s in texts]).astype(\"i\")\n",
    "\n",
    "def generate_dataset(nb_data, k):\n",
    "    out_k = k + 1    # +1 for carry digit\n",
    "    in_k = 2 * k + 2    # +1 for operator \"+\", +1 for \"=\"\n",
    "    a, b, c = generate(nb_data, k)\n",
    "    return datasets.TupleDataset(encode_in(a, b, in_k), encode_out(c, out_k))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 4\n",
    "a, b, c = generate(2, k)\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enc = encode_in(a, b, 2 * k + 2)\n",
    "enc.shape, enc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enc = encode_out(c, k + 1)\n",
    "enc.shape, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_in(x):\n",
    "    return \"\".join([input_alphabet[_] for _ in x])\n",
    "\n",
    "def decode_out(x):\n",
    "    return \"\".join([output_alphabet[_] for _ in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(Chain):\n",
    "    def __init__(self, unit, out_size):\n",
    "        super(Model, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(len(input_alphabet), unit)\n",
    "            self.l2 = L.LSTM(unit, unit)\n",
    "            self.l3 = L.Linear(unit, len(output_alphabet))\n",
    "            self.add_persistent(\"out_size\", out_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.l2.reset_state()\n",
    "        for i in range(x.shape[1]):\n",
    "            h = F.relu(self.l1(Variable(x[:, i, :])))\n",
    "            h = self.l2(h)\n",
    "        result = []\n",
    "        for i in range(self.out_size):\n",
    "            h = F.relu(h)\n",
    "            h = self.l2(h)\n",
    "            result.append(self.l3(h))\n",
    "        result = F.concat([F.reshape(_, (-1, 1, 11)) for _ in result], axis=1)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Calculator(Chain):\n",
    "    def __init__(self, predictor, use_gpu=None):\n",
    "        super(Calculator, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.predictor = predictor\n",
    "\n",
    "        global xp\n",
    "        if use_gpu is not None:\n",
    "            cuda.get_device(use_gpu).use()\n",
    "            self.predictor.to_gpu()\n",
    "            xp = cuda.cupy\n",
    "        else:\n",
    "            xp = np\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        t = Variable(t.flatten())\n",
    "        y = F.reshape(y, (-1, len(output_alphabet)))\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        accu = F.accuracy(y, t)\n",
    "        report({\"loss\": loss, \"accuracy\": accu, }, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = self.predictor(x)\n",
    "        return y.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomSerialIterator(iterators.SerialIterator):\n",
    "    def __init__(self, generator, batch_size, data_update_epoch, repeat=True, shuffle=True):\n",
    "        self.generator = generator\n",
    "        self.data_update_epoch = data_update_epoch\n",
    "        self.last_epoch = 0\n",
    "        super().__init__(self.generator(), batch_size, repeat, shuffle)\n",
    "\n",
    "    def next(self):\n",
    "        if self.last_epoch != self.epoch:\n",
    "            self.last_epoch = self.epoch\n",
    "            if self.epoch % self.data_update_epoch == 0:\n",
    "                self.dataset = self.generator()\n",
    "        minibatch = super().next()\n",
    "        return minibatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_args(cmdline=None):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", type=int)\n",
    "    parser.add_argument(\"--resume\", type=str)\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"result\")\n",
    "    parser.add_argument(\"--unit\", type=int, default=64)\n",
    "    parser.add_argument(\"--data_samples\", type=int, default=5000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=100)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10000)\n",
    "    parser.add_argument(\"--data_update_epoch\", type=int, default=10)\n",
    "    parser.add_argument(\"--number_width\", type=int, default=5)\n",
    "    args = parser.parse_args(cmdline)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(args):\n",
    "    out_k = args.number_width + 1    # +1 for carry digit\n",
    "    model = Calculator(Model(args.unit, out_k), args.gpu)\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_hook(evaluator):\n",
    "    model = evaluator.get_target(\"main\")\n",
    "    iterator = evaluator.get_iterator(\"main\")\n",
    "    iterator.reset()\n",
    "    minibatch = next(iterator)\n",
    "    x, t = evaluator.converter(minibatch)\n",
    "    y = model.predict(x)\n",
    "    print([\"{} {} -> {}\".format(\\\n",
    "        decode_in(x[i].argmax(axis=1)), decode_out(t[i]), decode_out(y[i].argmax(axis=1))\\\n",
    "    ) for i in range(len(x))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "#     train_set = generate_dataset(args.data_samples, args.number_width)\n",
    "#     train_iter = iterators.SerialIterator(train_set, batch_size=args.batch_size, repeat=True, shuffle=True)\n",
    "    train_iter = CustomSerialIterator(lambda: generate_dataset(args.data_samples, args.number_width), data_update_epoch=args.data_update_epoch, batch_size=args.batch_size, repeat=True, shuffle=True)\n",
    "    updater = training.StandardUpdater(train_iter, optimizer)\n",
    "    trainer = training.Trainer(updater, (args.epochs, \"epoch\"), out=args.outdir)\n",
    "\n",
    "    test_set = generate_dataset(33, args.number_width)\n",
    "    test_iter = iterators.SerialIterator(test_set, batch_size=3, repeat=False, shuffle=False)\n",
    "    trainer.extend(training.extensions.Evaluator(test_iter, model, eval_hook=eval_hook))\n",
    "\n",
    "    trainer.extend(training.extensions.LogReport(), trigger=(100, \"epoch\"))\n",
    "    trainer.extend(training.extensions.PrintReport([\"epoch\", \"main/loss\", \"validation/main/loss\", \"main/accuracy\", \"validation/main/accuracy\", ]), trigger=(1, \"epoch\"))\n",
    "    trainer.extend(training.extensions.snapshot(), trigger=(50, \"epoch\"))\n",
    "    if training.extensions.PlotReport.available():\n",
    "        trainer.extend(training.extensions.PlotReport([\"main/loss\", \"validation/main/loss\", ], \"epoch\", file_name=\"loss.png\"))\n",
    "        trainer.extend(training.extensions.PlotReport([\"main/accuracy\", \"validation/main/accuracy\", ], \"epoch\", file_name=\"accuracy.png\"))\n",
    "    trainer.extend(training.extensions.ProgressBar(), trigger=(10, \"epoch\"))\n",
    "    if args.resume is not None:\n",
    "        serializers.load_npz(args.resume, trainer)\n",
    "\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, args):\n",
    "    while True:\n",
    "        a = int(input(\"a=\"))\n",
    "        b = int(input(\"b=\"))\n",
    "        c = model.predict(encode_in([a], [b], args.number_width * 2 + 2))\n",
    "        print(\"predicted c={}\".format(decode_out(c[0].argmax(axis=1))))\n",
    "        print(\" computed c={}\".format(a + b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args([])\n",
    "#     args = parse_args([\"--resume\", \"result/snapshot_iter_500000\", \"--epochs\", \"10000\", ])\n",
    "    model, optimizer = init(args)\n",
    "    train(model, optimizer, args)\n",
    "    test(model, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
